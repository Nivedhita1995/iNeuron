{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65b77ea5",
   "metadata": {},
   "source": [
    "1. What is the difference between supervised and unsupervised learning? Give some examples to\n",
    "illustrate your point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d24cf0",
   "metadata": {},
   "source": [
    "-> The main difference between supervised and unsupervised learning lies in the presence or absence of labeled training data.\n",
    "\n",
    "-> Supervised learning\n",
    "Supervised learning algorithms are trained using labeled data and are guided by the provided labels to make predictions or classifications.\n",
    "\n",
    "Example: Email Spam Classification\n",
    "Consider a dataset of emails labeled as either \"spam\" or \"not spam.\" In supervised learning, we would train a model using this labeled data, with the input being the email content and the output being the spam or non-spam label. The model would learn patterns and features from the labeled examples and could then be used to classify new, unlabeled emails as either spam or not spam.\n",
    "\n",
    "-> Unsupervised learning\n",
    "In unsupervised learning, the algorithm explores the structure, patterns, and relationships within the data without any specific guidance or labeled information. The goal is to discover hidden patterns, groupings, or similarities in the data.\n",
    "\n",
    "Example: Customer Segmentation\n",
    "Consider a dataset containing customer purchase histories but without any labels.\n",
    "The algorithm would analyze the patterns in the data and group customers based on similarities, revealing potential segments such as \"price-sensitive shoppers,\" \"high-value customers,\" or \"occasional buyers.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e6eef5",
   "metadata": {},
   "source": [
    "2. Mention a few unsupervised learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68cd8ac",
   "metadata": {},
   "source": [
    "Applications of unsupervised learning:\n",
    "\n",
    "Clustering: Applications include customer segmentation, image segmentation, document clustering, and anomaly detection.\n",
    "Dimensionality Reduction: Helpful for data visualization, feature extraction, and preprocessing before applying supervised learning algorithms.\n",
    "Market Basket Analysis: It helps identify frequent itemsets or combinations of items that tend to be purchased together, enabling businesses to optimize product placement, recommend related items, or implement targeted marketing strategies.\n",
    "Anomaly Detection:  It is applied in various domains such as fraud detection in financial transactions, network intrusion detection, equipment failure detection, and health monitoring systems.\n",
    "Image and Video Processing: They assist in tasks like object recognition, scene understanding, image annotation, and video summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c570ba74",
   "metadata": {},
   "source": [
    "3. What are the three main types of clustering methods? Briefly describe the characteristics of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13f1647",
   "metadata": {},
   "source": [
    "The three main types of clustering methods are:\n",
    "-> Hierarchical Clustering:\n",
    "Hierarchical clustering creates a hierarchical structure of clusters by iteratively merging or splitting clusters.\n",
    "It does not require a predefined number of clusters and can produce a dendrogram that shows the relationship between clusters.\n",
    "It can be agglomerative, starting with each data point as a separate cluster and progressively merging them, or divisive, starting with all data points in a single cluster and recursively splitting them.\n",
    "-> Partitioning Clustering:\n",
    "Partitioning clustering aims to divide the data into non-overlapping clusters, where each data point belongs to only one cluster.\n",
    "It requires the user to specify the number of clusters in advance.\n",
    "The most popular partitioning clustering algorithm is k-means, which iteratively assigns data points to clusters by minimizing the sum of squared distances within each cluster.\n",
    "-> Density-Based Clustering:\n",
    "Density-based clustering groups data points based on their density in the data space.\n",
    "It identifies regions of high density separated by regions of low density.\n",
    "The key concept is defining a density-reachable relationship, where a point is considered part of a cluster if it has sufficient nearby neighbors.\n",
    "Density-based clustering algorithms, such as DBSCAN (Density-Based Spatial Clustering of Applications with Noise), can discover clusters of arbitrary shape and handle noise effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38077e56",
   "metadata": {},
   "source": [
    "4. Explain how the k-means algorithm determines the consistency of clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7db19b",
   "metadata": {},
   "source": [
    "The k-means algorithm does not directly determine the consistency of clustering. \n",
    "Instead, it aims to find a clustering solution that minimizes the within-cluster sum of squared distances, also known as the inertia or total within-cluster variance. \n",
    "The lower the inertia value, the more consistent the clustering is in terms of compactness.\n",
    "\n",
    "-> The k-means algorithm follows an iterative process to optimize the clustering solution. \n",
    "Initialization: The algorithm starts by randomly initializing k cluster centroids, where k is the predefined number of clusters.\n",
    "Assignment/ expectation: Each data point is assigned to the cluster whose centroid is closest to it based on the Euclidean distance metric. \n",
    "Update/ maximization: After all data points are assigned to clusters, the centroid of each cluster is updated by computing the mean of the data points within that cluster. \n",
    "Iteration: Assignment and Update steps are repeated iteratively until convergence. Convergence occurs when the cluster assignments and centroids no longer change significantly, or a maximum number of iterations is reached.\n",
    " The final clustering solution is obtained when the algorithm converges. Each data point belongs to the cluster associated with the nearest centroid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faf474f",
   "metadata": {},
   "source": [
    "5. With a simple illustration, explain the key difference between the k-means and k-medoids\n",
    "algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080b7fd1",
   "metadata": {},
   "source": [
    "The key difference between the k-means and k-medoids algorithms lies in the way they determine the cluster centers or centroids.\n",
    "\n",
    "Consider a simple illustration with five data points: [2,4,7,12,15]. We want to perform 2-means clustering (k=2) using the k-means and k-medoids algorithms.\n",
    "\n",
    "k=means:\n",
    "We randomly initialize two centroids, let's say C1=4 and C2=12.\n",
    "Assign each data point to the nearest centroid. Let's assume the assignment is [C1,C1,C2,C2,C2].\n",
    "Update the centroids by calculating the mean of the data points in each cluster. The new centroids would be C1=3 and C2=11 (mean of [2,4] and [7,12,15] respectively).\n",
    "Repeat Assign and Update until convergence.\n",
    "\n",
    "k-medoids:\n",
    "We randomly select two medoids from the data points, let's say M1=2 and M2=12.\n",
    "Assign each data point to the nearest medoid. Let's assume the assignment is [M1, M1, M2, M2, M2].\n",
    "For each cluster, calculate the dissimilarity (e.g., Euclidean distance) between each data point and all other points in the cluster. Select the point with the minimum sum of dissimilarities as the new medoid.\n",
    "Repeat Assign and Update until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af43691",
   "metadata": {},
   "source": [
    "6. What is a dendrogram, and how does it work? Explain how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bafff7c",
   "metadata": {},
   "source": [
    "A dendrogram is a tree-like diagram that represents the hierarchical clustering of data points. It visually displays the relationships and similarities between clusters and individual data points. \n",
    "\n",
    "Construction and Working of a dendrogram:\n",
    "-> Calculate Euclidean distance or Manhattan distance.\n",
    "-> Choose a linkage method that determines how the distances between clusters are calculated.\n",
    "-> Initially, each data point is considered as a separate cluster.\n",
    "-> Combine the two closest clusters based on the chosen linkage method. The distance between clusters is determined by the pairwise distance between their data points. This is repeated until all data points are merged into a single cluster.\n",
    "-> As clusters are merged, a tree-like structure called a dendrogram is formed.\n",
    "-> The vertical height of the dendrogram represents the distance or dissimilarity between clusters at each merge step. The longer the vertical distance, the more dissimilar the clusters are.\n",
    "-> The horizontal axis of the dendrogram represents the data points in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac24cf77",
   "metadata": {},
   "source": [
    "7. What exactly is SSE? What role does it play in the k-means algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc09ed83",
   "metadata": {},
   "source": [
    "SSE stands for Sum of Squared Errors or Sum of Squared Distances. \n",
    "It is a metric used to measure the quality or compactness of clusters in the k-means algorithm. \n",
    "In the k-means algorithm, the objective is to minimize the SSE. The algorithm iteratively assigns data points to clusters and updates the cluster centroids to minimize the total SSE. \n",
    "The lower the SSE, the more compact and similar the data points within each cluster.\n",
    "The SSE plays a crucial role in the k-means algorithm in two main ways:\n",
    "Assignment/ expectation: Each data point is assigned to the cluster whose centroid is closest to it based on the Euclidean distance metric. \n",
    "Update/ maximization: After all data points are assigned to clusters, the centroid of each cluster is updated by computing the mean of the data points within that cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a952ab65",
   "metadata": {},
   "source": [
    "8. With a step-by-step algorithm, explain the k-means procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b0f086",
   "metadata": {},
   "source": [
    "A step-by-step algorithm to explain the k-means procedure:\n",
    "\n",
    "-> Choose the number of clusters, k, that you want to create.\n",
    "\n",
    "-> Initialize k cluster centroids randomly or based on a specific strategy. Each centroid represents the center point of a cluster.\n",
    "\n",
    "-> Repeat the Assign and Update steps until convergence (when the centroids no longer change significantly or a maximum number of iterations is reached):\n",
    "\n",
    "-> Once the algorithm converges, you have obtained k clusters with their respective centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a929ff",
   "metadata": {},
   "source": [
    "9. In the sense of hierarchical clustering, define the terms single link and complete link."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97701e6a",
   "metadata": {},
   "source": [
    "In the context of hierarchical clustering, \"single link\" and \"complete link\" are two commonly used methods for calculating the distance or dissimilarity between clusters. \n",
    "These methods determine how the proximity between clusters is measured when constructing a hierarchical clustering tree or dendrogram.\n",
    "\n",
    "-> Single Link:\n",
    "Single link, also known as the \"nearest neighbor\" or \"minimum method,\" calculates the distance between two clusters based on the shortest distance between any two points from the two clusters.\n",
    "\n",
    "-> Complete Link:\n",
    "Complete link, also known as the \"furthest neighbor\" or \"maximum method,\" calculates the distance between two clusters based on the longest distance between any two points from the two clusters.\n",
    "\n",
    "Both single link and complete link are agglomerative methods, meaning they start with each data point as a separate cluster and iteratively merge the closest or most dissimilar clusters until a desired number of clusters is reached. \n",
    "The choice between single link and complete link depends on the characteristics of the dataset and the desired clustering outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ae5e7c",
   "metadata": {},
   "source": [
    "10. How does the apriori concept aid in the reduction of measurement overhead in a business\n",
    "basket analysis? Give an example to demonstrate your point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb7607",
   "metadata": {},
   "source": [
    "The Apriori concept aids in the reduction of measurement overhead in business basket analysis by efficiently identifying and focusing on frequent itemsets, thereby reducing the number of itemsets that need to be evaluated. \n",
    "\n",
    "Consider a dataset of customer transactions in a grocery store, and we want to perform basket analysis to find frequently co-occurring items in customer baskets.\n",
    "The Apriori algorithm starts by identifying frequent individual items by scanning the dataset once. \n",
    "Then, it iteratively generates candidate itemsets by combining frequent itemsets of lower lengths. \n",
    "It prunes the itemsets that are determined to be infrequent based on the minimum support threshold.\n",
    "\n",
    "Assuming minimum support threshold of 5%.The Apriori algorithm would initially identify the frequent individual items, such as {milk}, {bread}, {eggs}, and {cheese}. Then, it would generate candidate itemsets of length two, such as {milk, bread}, {milk, eggs}, {milk, cheese}, and so on. It would evaluate the support of these candidate itemsets and prune the ones that do not meet the minimum support threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f425e49",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
