{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "092d8744",
   "metadata": {},
   "source": [
    "1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2\n",
    "and that the first set of random centroid is 15, 32, and that the second set is 12, 30.\n",
    "a. Using the k-means method, create two clusters for each set of centroid described above.\n",
    "b. For each set of centroid values, calculate the SSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d70579f",
   "metadata": {},
   "source": [
    "Given:\n",
    "    Data points= {5, 10, 15, 20, 25, 30, 35}\n",
    "    k=2\n",
    "    First Set of centroid: 15, 32\n",
    "    Second Set of Centroids: 12, 30\n",
    "a. Using the k-means method, create two clusters for each set of centroid described above.      \n",
    "Steps:\n",
    "    Initialization: \n",
    "    Assign data points to the nearest centroid based on Euclidean distance.\n",
    "    Calculate the mean of each cluster to update the centroid coordinates.\n",
    "    Repeat until convergence.\n",
    "    \n",
    "First Iteration:\n",
    "    Assign Data points {5, 10, 15, 20, 25, 30, 35}\n",
    "    Data points {5, 10, 15} are closer to centroid 15.\n",
    "    Data points {20, 25, 30, 35} are closer to centroid 32.\n",
    "    \n",
    "    Update the centroid coordinates:\n",
    "    Cluster 1: Mean = (5 + 10 + 15) / 3 = 10\n",
    "    Cluster 2: Mean = (20 + 25 + 30 + 35) / 4 = 27.5.\n",
    "        \n",
    "Similarly Second Iteration:\n",
    "    Data points {5, 10} are closer to centroid 10.\n",
    "    Data points {15,20, 25, 30, 35} are closer to centroid 27.5.\n",
    "    \n",
    "Final Clusters:\n",
    "Cluster 1: {5, 10, 7.5}\n",
    "Cluster 2: {15, 20, 25, 30, 35, 25}\n",
    "    \n",
    "b. For each set of centroid values, calculate the SSE.\n",
    "\n",
    "SSE = Σ(distance(point, centroid)^2)\n",
    "\n",
    "For the First Set of Centroids: 15, 32\n",
    "SSE = (10^2 + 5^2 + 0^2) + (12^2 + 7^2 + 2^2 + 3^2)\n",
    "SSE = 125 + 182\n",
    "SSE = 307\n",
    "\n",
    "For the Second Set of Centroids: 12, 30\n",
    "SSE = (7^2 + 2^2 + 3^2) + (8^2 + 5^2 + 0^2 + 5^2)\n",
    "SSE = 62 + 114\n",
    "SSE = 176"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8894aea",
   "metadata": {},
   "source": [
    "2. Describe how the Market Basket Research makes use of association analysis concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba910e4f",
   "metadata": {},
   "source": [
    "-> Market Basket Analysis (MBA) makes use of association analysis concepts to uncover meaningful relationships and patterns among items that are frequently purchased together. \n",
    "-> It is a technique widely used in retail and e-commerce industries to understand customer behavior, improve marketing strategies, and enhance sales.\n",
    "-> The Market Basket Research makes use of association analysis concepts in:\n",
    "Frequent Itemsets, Association Rules, Support and Confidence Thresholds and Rule Evaluation and Interpretation.\n",
    "->Market Basket Analysis leverages association analysis concepts to uncover hidden relationships in transactional data, enabling businesses to understand customer behavior, optimize product placement, create personalized recommendations, and improve overall sales and customer satisfaction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26516b3",
   "metadata": {},
   "source": [
    "3. Give an example of the Apriori algorithm for learning association rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd925681",
   "metadata": {},
   "source": [
    "Example of an example of the Apriori algorithm for learning association rules:\n",
    "Suppose we have the following dataset that has various transactions, and from this dataset, we need to find the frequent itemsets and generate the association rules using the Apriori algorithm:\n",
    "\n",
    "TID         ITEMS SETS\n",
    "T1           A,B\n",
    "T2           B,D\n",
    "T3           B,C\n",
    "T4           A,B,D\n",
    "T5           A,C\n",
    "T6           B,C\n",
    "T7           A,C\n",
    "T8           A,B,C,E\n",
    "T9           A,B,C\n",
    "\n",
    "Step-1: Calculating C1 and L1:\n",
    "\n",
    "Itemset        Support_Count\n",
    "A               6\n",
    "B               7\n",
    "C               5\n",
    "D               2\n",
    "E               1\n",
    "\n",
    "Step-2: Candidate Generation C2, and L2:\n",
    "\n",
    "Itemset        Support_Count\n",
    "{A,B}          4\n",
    "{A,C}          4\n",
    "{B,C}          4\n",
    "{B,D}          2\n",
    "\n",
    "Step-3: Candidate generation C3, and L3:\n",
    "\n",
    "Itemset        Support_Count\n",
    "{A,B,C}        2\n",
    "{B,C,D}        1\n",
    "\n",
    "Step-4: Finding the association rules for the subsets:\n",
    "\n",
    "Rules\tSupport\tConfidence\n",
    "A ^B → C\t2\tSup{(A ^B) ^C}/sup(A ^B)= 2/4=0.5=50%\n",
    "B^C → A\t2\tSup{(B^C) ^A}/sup(B ^C)= 2/4=0.5=50%\n",
    "A^C → B\t2\tSup{(A ^C) ^B}/sup(A ^C)= 2/4=0.5=50%\n",
    "C→ A ^B\t2\tSup{(C^( A ^B)}/sup(C)= 2/5=0.4=40%\n",
    "A→ B^C\t2\tSup{(A^( B ^C)}/sup(A)= 2/6=0.33=33.33%\n",
    "B→ B^C\t2\tSup{(B^( B ^C)}/sup(B)= 2/7=0.28=28%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b981e",
   "metadata": {},
   "source": [
    "4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metric\n",
    "is used to decide when to end the iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bd0571",
   "metadata": {},
   "source": [
    "-> in single linkage hierarchical clustering, the distance between two clusters is defined as the shortest distance between two points in each cluster. \n",
    "-> For most common hierarchical clustering software, the default distance measure is the Euclidean distance. This is the square root of the sum of the square differences. However, for gene expression, correlation distance is often used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6444cf1c",
   "metadata": {},
   "source": [
    "5. In the k-means algorithm, how do you recompute the cluster centroids?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8f38ff",
   "metadata": {},
   "source": [
    "Steps to recompute the cluster centroids:\n",
    "-> Step:1 Choose the number of clusters k. The first step in k-means is to pick the number of clusters, k.\n",
    "-> Step:2 Select k random points from the data as centroids. ...\n",
    "-> Step:3 Assign all the points to the closest cluster centroid. ...\n",
    "-> Step:4 Recompute the centroids of newly formed clusters. ...\n",
    "Repeat steps 3 and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb56c79",
   "metadata": {},
   "source": [
    "6. At the start of the clustering exercise, discuss one method for determining the required number of\n",
    "clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a4dac2",
   "metadata": {},
   "source": [
    "Silhouette method\n",
    "Optimization techniques such as genetic algorithms are useful in determining the number of clusters that gives rise to the largest silhouette. It is also possible to re-scale the data in such a way that the silhouette is more likely to be maximized at the correct number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ba607",
   "metadata": {},
   "source": [
    "7. Discuss the k-means algorithm's advantages and disadvantages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffaa68c",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "    \n",
    "It is very easy to understand and implement.\n",
    "\n",
    "If we have large number of variables then, K-means would be faster than Hierarchical clustering.\n",
    "\n",
    "On re-computation of centroids, an instance can change the cluster.\n",
    "\n",
    "Tighter clusters are formed with K-means as compared to Hierarchical clustering.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "It is a bit difficult to predict the number of clusters i.e. the value of k.\n",
    "\n",
    "Output is strongly impacted by initial inputs like number of clusters (value of k).\n",
    "\n",
    "Order of data will have strong impact on the final output.\n",
    "\n",
    "It is very sensitive to rescaling. If we will rescale our data by means of normalization or standardization, then the output will completely change.final output.\n",
    "\n",
    "It is not good in doing clustering job if the clusters have a complicated geometric shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca26bd6",
   "metadata": {},
   "source": [
    "9. During your study, you discovered seven findings, which are listed in the data points below. Using\n",
    "the K-means algorithm, you want to build three clusters from these observations. The clusters C1,\n",
    "C2, and C3 have the following findings after the first iteration:\n",
    "\n",
    "C1: (2,2), (4,4), (6,6) \n",
    "\n",
    "C2: (0,4), (4,0), (0,4)\n",
    "\n",
    "C3: (5,5) and (9,9)\n",
    "\n",
    "What would the cluster centroids be if you were to run a second iteration? What would this\n",
    "clustering's SSE be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ee059",
   "metadata": {},
   "source": [
    "Finding centroid for data points in cluster C1 = ((2+4+6)/3, (2+4+6)/3) = (4, 4)Finding centroid for data points in cluster C2 = ((0+4)/2, (4+0)/2) = (2, 2)Finding centroid for data points in cluster C3 = ((5+9)/2, (5+9)/2) = (7, 7)Hence, C1: (4,4), C2: (2,2), C3: (7,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92f0f18",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
