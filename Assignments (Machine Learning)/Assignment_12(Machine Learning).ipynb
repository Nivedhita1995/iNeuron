{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7426f6eb",
   "metadata": {},
   "source": [
    "1. What is prior probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5796f70a",
   "metadata": {},
   "source": [
    "Prior probability, also known as prior belief or prior distribution, refers to the initial probability assigned to an event or hypothesis before considering any new evidence or data. \n",
    "It represents the knowledge or belief about the likelihood of an event or hypothesis before any empirical observations or experiments are conducted.\n",
    "Prior probability is often used in Bayesian statistics, where it serves as the starting point for updating probabilities based on new evidence using Bayes' theorem.\n",
    "Example:\n",
    "Consider a medical scenario where a patient visits a doctor and undergoes a medical test to determine if they have a certain disease. Before conducting the test, the doctor has some prior knowledge or belief about the patient's probability of having the disease based on factors such as symptoms, medical history, and risk factors.\n",
    "In this example represents the doctor's initial belief about the patient's condition before any diagnostic information is available. It provides a starting point for making informed decisions and guiding further investigation or treatment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e6a63f",
   "metadata": {},
   "source": [
    "2. What is posterior probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fded55",
   "metadata": {},
   "source": [
    "Posterior probability, in the context of Bayesian statistics, refers to the updated probability of an event or hypothesis after considering new evidence or data. \n",
    "It is calculated by combining the prior probability with the likelihood of the observed data using Bayes' theorem.\n",
    "\n",
    "Example:\n",
    "Continue with the medical scenario mentioned earlier, where a doctor is assessing the probability of a patient having a certain disease. The doctor initially assigns a prior probability of 10% based on their prior knowledge and assessment.\n",
    "Prior Probability (P(Disease)) = 0.10\n",
    "Likelihood (P(Positive Test Result | Disease)) = 0.80\n",
    "Using Bayes' theorem:\n",
    "\n",
    "Posterior Probability (P(Disease | Positive Test Result)) = (P(Disease) * P(Positive Test Result | Disease)) / P(Positive Test Result)\n",
    "\n",
    "The denominator P(Positive Test Result) is calculated by considering the probabilities of both a positive test result with the disease and a positive test result without the disease.\n",
    "\n",
    "By substituting the values into the formula and performing the calculations, the doctor can obtain the updated posterior probability of the patient having the disease given the positive test result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a9ad9b",
   "metadata": {},
   "source": [
    "3. What is likelihood probability? Give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eb3b8e",
   "metadata": {},
   "source": [
    "Likelihood, in the context of statistics, refers to the probability of observing a specific set of data or evidence given a particular hypothesis or model.\n",
    "The likelihood function provides a way to assess the fit between the observed data and the hypothesized model, and it is commonly used in maximum likelihood estimation and Bayesian inference.\n",
    "\n",
    "Example: \n",
    "Estimating the probability of heads in a biased coin toss.\n",
    "We conduct an experiment where we flip the coin 10 times and observe the following sequence of outcomes: H T H H T H H T T T.\n",
    "Given the probability of heads being 0.3. Assuming each flip is independent, we can calculate the likelihood as follows:\n",
    "\n",
    "Likelihood (p = 0.3) = P(H T H H T H H T T T | p = 0.3)\n",
    "\n",
    "Since each flip has a probability of 0.3 for heads (H) and 0.7 for tails (T), we can multiply these probabilities for each observed outcome in the sequence. The likelihood is the product of these probabilities:\n",
    "\n",
    "Likelihood (p = 0.3) = 0.3 * 0.7 * 0.3 * 0.3 * 0.7 * 0.3 * 0.3 * 0.7 * 0.7 * 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168178ed",
   "metadata": {},
   "source": [
    "4. What is Naïve Bayes classifier? Why is it named so?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8da56cb",
   "metadata": {},
   "source": [
    "The Naïve Bayes classifier is a popular classification algorithm that is based on Bayes' theorem and assumes independence among the features (predictors) in a dataset. \n",
    "It is named \"Naïve\" because it makes a strong assumption of feature independence, which is often an oversimplification in real-world scenarios.\n",
    "The Naïve Bayes classifier calculates the probability of a data point belonging to a particular class by combining prior probabilities (prior knowledge or beliefs about the class frequencies) and the likelihood of the observed features given that class. The class with the highest probability is then assigned as the predicted class.\n",
    "The name \"Naïve Bayes\" comes from the fact that it assumes feature independence, meaning that the presence or absence of one feature is considered unrelated to the presence or absence of other features. This assumption allows for simple and efficient computation of probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e850c7",
   "metadata": {},
   "source": [
    "5. What is optimal Bayes classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085256f3",
   "metadata": {},
   "source": [
    "The optimal Bayes classifier, also known as the Bayes optimal classifier or Bayes optimal decision rule, is a theoretical framework for classification that provides the best possible classification accuracy given the underlying true distribution of the data.\n",
    "The optimal Bayes classifier is derived from Bayes' theorem and uses the posterior probabilities of different classes to make classification decisions. It assigns a data point to the class with the highest posterior probability. Mathematically, the optimal Bayes classifier can be expressed as:\n",
    "\n",
    "Decision = argmax(P(Class | Data))\n",
    "\n",
    "where P(Class | Data) is the posterior probability of a particular class given the observed data, and the argmax operator selects the class that maximizes this probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47c114e",
   "metadata": {},
   "source": [
    "6. Write any two features of Bayesian learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5774ee3b",
   "metadata": {},
   "source": [
    "Bayesian learning methods, also known as Bayesian inference or probabilistic modeling, have several distinctive features that set them apart from other learning approaches.\n",
    "Probabilistic Framework: Bayesian learning methods are based on a probabilistic framework, where probabilities and probability distributions are used to represent uncertainty.\n",
    "Prior Knowledge Incorporation: Bayesian methods provide a systematic way to incorporate prior knowledge or beliefs into the learning process. \n",
    "Bayesian Inference: Bayesian learning involves updating prior beliefs based on observed data using Bayes' theorem.\n",
    "Model Flexibility: Bayesian learning methods offer flexibility in model specification. They can handle complex models with multiple parameters, hierarchical structures, and latent variables. \n",
    "Sequential Learning and Updating: Bayesian learning allows for sequential learning and updating of models as new data becomes available.\n",
    "Regularization and Overfitting Control: Bayesian methods naturally incorporate regularization techniques, such as prior distributions, to prevent overfitting.\n",
    "Uncertainty Quantification: Bayesian learning provides a natural way to quantify uncertainty in predictions. \n",
    "Model Comparison and Selection: Bayesian methods provide tools for model comparison and selection based on principles such as model evidence or Bayesian model selection criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c9d805",
   "metadata": {},
   "source": [
    "7. Define the concept of consistent learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33893c8",
   "metadata": {},
   "source": [
    "In machine learning, consistent learners are algorithms or models that are designed to converge to the true underlying pattern or function generating the data as the size of the training data increases.\n",
    "A consistent learner will produce increasingly accurate predictions or estimations as more data is provided.\n",
    "It is important to note that the concept of consistency is specific to the learning algorithm or model being used. Different algorithms or models may have different consistency properties depending on their underlying assumptions and characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166d9b2a",
   "metadata": {},
   "source": [
    "8. Write any two strengths of Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe37bac",
   "metadata": {},
   "source": [
    "Two strengths of the Bayes classifier are:\n",
    "\n",
    "Handling of Uncertainty: \n",
    "The Bayes classifier provides a probabilistic framework that explicitly models and quantifies uncertainty. \n",
    "It assigns probabilities to different classes, allowing for a more nuanced understanding of the likelihood of a data point belonging to a particular class. This is especially useful when making decisions or evaluating the risk associated with different outcomes. \n",
    "The Bayes classifier's ability to capture uncertainty is valuable in many real-world applications.\n",
    "\n",
    "Efficient and Scalable: \n",
    "The Bayes classifier, particularly the Naïve Bayes variant, is known for its simplicity and efficiency. \n",
    "It requires a relatively small amount of training data to estimate the necessary probabilities and can work well even with limited training samples. The computational cost of the Bayes classifier is generally low, making it computationally efficient and scalable to large datasets. This characteristic makes it suitable for real-time and resource-constrained applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e172962c",
   "metadata": {},
   "source": [
    "9. Write any two weaknesses of Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e36982",
   "metadata": {},
   "source": [
    "Two weaknesses of the Bayes classifier are:\n",
    "\n",
    "Independence Assumption: \n",
    "The Naïve Bayes classifier, which is a variant of the Bayes classifier, assumes independence among the features (predictors) in the dataset. This assumption is often an oversimplification in real-world scenarios, as features may exhibit dependencies or correlations with each other. The independence assumption can lead to inaccurate predictions when there are significant interdependencies among the features. While more advanced variants of the Bayes classifier can relax this assumption to some extent, the issue of feature interdependencies remains a limitation of the Bayes classifier.\n",
    "\n",
    "Sensitivity to Feature Quality: \n",
    "The performance of the Bayes classifier heavily relies on the quality and relevance of the features used for classification. If important features are missing or irrelevant features are included, it can negatively impact the classifier's accuracy. The Bayes classifier treats all features equally and assigns equal weight to them. If certain features have more discriminative power than others, the Bayes classifier may not fully exploit their potential, leading to suboptimal performance. Feature selection and engineering techniques are crucial to address this weakness and improve the performance of the Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3476a7ea",
   "metadata": {},
   "source": [
    "10. Explain how Naïve Bayes classifier is used for\n",
    "\n",
    "1. Text classification\n",
    "\n",
    "2. Spam filtering\n",
    "\n",
    "3. Market sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ee8653",
   "metadata": {},
   "source": [
    "1. Text Classification:\n",
    "\n",
    "In text classification, the goal is to categorize documents or text data into predefined categories or classes. The Naïve Bayes classifier can be trained on a labeled dataset where each document is associated with a specific category. The classifier learns the probability distribution of words or features in each category using the training data.\n",
    "During the classification phase, the Naïve Bayes classifier uses the learned probabilities to calculate the posterior probability of each category given a new document. \n",
    "It applies Bayes' theorem to calculate the likelihood of observing the words in the document given the category, multiplied by the prior probability of the category. The classifier assigns the document to the category with the highest posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f72a64c",
   "metadata": {},
   "source": [
    "2. Spam filtering\n",
    "\n",
    "The classifier can be trained on a dataset consisting of labeled emails or messages, where each message is labeled as either spam or non-spam (ham). The Naïve Bayes classifier learns the probability distribution of words or features in spam and non-spam messages.\n",
    "During the filtering process, the Naïve Bayes classifier calculates the posterior probability of a message being spam or non-spam given its features (words, patterns, etc.). It uses Bayes' theorem to estimate the likelihood of observing the features in a spam or non-spam message, multiplied by the prior probabilities of spam and non-spam. The classifier then assigns a spam or non-spam label to the message based on the category with the highest posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c562cdcc",
   "metadata": {},
   "source": [
    "3. Market sentiment analysis\n",
    "\n",
    "To perform market sentiment analysis, the Naïve Bayes classifier is trained on a labeled dataset of textual data with sentiment annotations. The classifier learns the probability distribution of words or features in each sentiment category. \n",
    "During the analysis, the classifier calculates the posterior probability of each sentiment category given the features in the text. It assigns the text to the sentiment category with the highest posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e33170",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
